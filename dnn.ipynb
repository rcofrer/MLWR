{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dnn.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"I1Y9j8FGp5_j"},"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import gc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import scipy\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpZwHWvobzob","executionInfo":{"status":"ok","timestamp":1612717633423,"user_tz":-60,"elapsed":66066,"user":{"displayName":"Daniel Santiago Cuervo Gomez","photoUrl":"","userId":"00610344567404782826"}},"outputId":"c7adf1b8-47e1-405c-e6cf-e3421d8daef5"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d24HmfwGahPL"},"source":["with open('/content/gdrive/My Drive/Notebooks Colab/kaggleDF.pickle', 'rb') as handle:\n","    dfTrain, dfTest = pickle.load(handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2RvTYHAC_Hcc"},"source":["categoricalColumns = list(dfTrain.select_dtypes(exclude='float32').columns)\n","numericalColumns = list(dfTrain.select_dtypes(include='float32').columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r6Bgtgv1syIP"},"source":["# Independently integer encode train and test sets"]},{"cell_type":"code","metadata":{"id":"gHeUUF0m8CO8"},"source":["def factorize(train, test, col):\n","    if hasattr(train[col], 'cat'):\n","        train[col] = train[col].astype('object')\n","        test[col] = test[col].astype('object')\n","    encodedTrain, uniques = train[col].factorize(sort=True)\n","    # MAKE SMALLEST LABEL 1, RESERVE 0\n","    maxEncodedVal = encodedTrain.max()\n","    encodedTrain = np.where(encodedTrain == -1, maxEncodedVal + 1, encodedTrain)\n","    train[col] = encodedTrain\n","    encodingDict = {}\n","    for encodedVal, previousVal in enumerate(uniques):\n","        encodingDict[previousVal] = encodedVal\n","    # possibly non-exhaustvie mapping: \n","    # https://stackoverflow.com/questions/42529454/using-map-for-columns-in-a-pandas-dataframe\n","    test[col].fillna(-1, inplace = True)\n","    test[col] = test[col].apply(lambda x: maxEncodedVal + 2 if x not in uniques and x != -1 else x)\n","    test[col] = test[col].map(encodingDict).fillna(test[col])\n","    # now handling the values which were not in the train set\n","    # just make them any integer not used already, e.g. max + 2, LGBM doesn't care\n","    test[col] = np.where(test[col] == -1, maxEncodedVal + 1, test[col])\n","    test[col] = test[col].astype('uint32')\n","\n","for col in categoricalColumns:\n","    if col != \"HasDetections\":\n","        factorize(dfTrain, dfTest, col)\n","        dfTrain[col] = dfTrain[col].astype('category')\n","        dfTest[col] = dfTest[col].astype('category')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NvZSsqXds7Yg"},"source":["# Fill NaN values with the mean"]},{"cell_type":"code","metadata":{"id":"pDa_ztvE5dlI"},"source":["for col in numericalColumns:\n","    dfTrain[col].fillna(dfTrain[col].mean(), inplace=True)\n","    dfTest[col].fillna(dfTrain[col].mean(), inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xh0Z-NEuteCl"},"source":["# Save clean dataset"]},{"cell_type":"code","metadata":{"id":"TX9ciWzXBOrM"},"source":["with open('/content/gdrive/My Drive/Notebooks Colab/cleanKaggleDF.pickle', 'wb') as handle:\n","    pickle.dump((dfTrain, dfTest), handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aq_CxCVxtoBq"},"source":["# Split the train set on train and validation sets"]},{"cell_type":"code","metadata":{"id":"cv0c8AOQ9e95"},"source":["X = dfTrain.copy().drop('HasDetections', 1)\n","y = dfTrain.copy()['HasDetections']\n","del dfTrain\n","xTrain, xVal, yTrain, yVal = train_test_split(X, y, test_size=0.01, stratify=y, random_state=11)\n","xTrain.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iyAX9pA_t4mJ"},"source":["# Define the embedding dimensions"]},{"cell_type":"code","metadata":{"id":"Rfdq6YAb_7Mi"},"source":["embeddedCols = {n: len(col.cat.categories) for n, col in dfTrain.items() if n in categoricalColumns and n!= 'HasDetections' and len(col.cat.categories) > 2}\n","embeddedColNames = embeddedCols.keys()\n","nCont = len(dfTrain.columns) - 1 - len(embeddedCols) # Number of numerical columns\n","embeddingSizes = [(nCategories, min(50, (nCategories + 1)//2)) for _, nCategories in embeddedCols.items()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6Q5k-xKuMHD"},"source":["# Data handling utilities"]},{"cell_type":"code","metadata":{"id":"_QPj0nYQDsuA"},"source":["class MalwareDataset(Dataset):\n","    def __init__(self, X, y, embeddedColNames):\n","        X = X.copy()\n","        self.x1 = X.loc[:,embeddedColNames].copy().values.astype(np.int64) # Categorical columns\n","        self.x2 = X.drop(columns=embeddedColNames).copy().values.astype(np.float32) # Numerical columns\n","        self.y = y.values.astype(np.float32)\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.x1[idx], self.x2[idx], self.y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvPuoGseD-LB"},"source":["trainDF = MalwareDataset(xTrain, yTrain, embeddedColNames)\n","testDF = MalwareDataset(xVal, yVal, embeddedColNames)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NE02qpfEXD8"},"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBhXaZ-yEkfE"},"source":["def toDevice(data, device):\n","    if isinstance(data, (list,tuple)):\n","        return [toDevice(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader(DataLoader):\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        for b in self.dl: \n","            yield toDevice(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZttZ8tMuPIT"},"source":["# Model definition"]},{"cell_type":"code","metadata":{"id":"dbCs2X1hEz_D"},"source":["class MalwareModel(nn.Module):\n","    def __init__(self, embbedingSizes, nCont):\n","        super().__init__()\n","        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embbedingSizes])\n","        nEmb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n","        self.nEmb, self.nCont = nEmb, nCont\n","        self.lin1 = nn.Linear(self.nEmb + self.nCont, 200)\n","        self.lin2 = nn.Linear(200, 70)\n","        self.lin3 = nn.Linear(70, 1)\n","        self.bn1 = nn.BatchNorm1d(self.nCont)\n","        self.bn2 = nn.BatchNorm1d(200)\n","        self.bn3 = nn.BatchNorm1d(70)\n","        self.embDrop = nn.Dropout(0.6)\n","        self.drops = nn.Dropout(0.3)\n","        \n","\n","    def forward(self, xCat, xCont):\n","        x = [e(xCat[:,i]) for i,e in enumerate(self.embeddings)]\n","        x = torch.cat(x, 1)\n","        x = self.embDrop(x)\n","        x2 = self.bn1(xCont)\n","        x = torch.cat([x, x2], 1)\n","        x = torch.relu(self.bn2(self.lin1(x)))\n","        x = self.drops(x)\n","        x = torch.relu(self.bn3(self.lin2(x)))\n","        x = self.drops(x)\n","        x = torch.sigmoid(self.lin3(x))\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUwSPgvpFQGO"},"source":["model = MalwareModel(embeddingSizes, nCont)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aymu76xouXmy"},"source":["# Training loop functions:"]},{"cell_type":"code","metadata":{"id":"CXm72aVlHyxZ"},"source":["def getOptimizer(model, lr = 1e-4, wd=0.0):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n","    return optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jl4zPyQ5ItY-"},"source":["def trainModel(model, criterion, optim, trainDL):\n","    model.train()\n","    total = 0\n","    sum_loss = 0\n","    for x1, x2, y in trainDL:\n","        batch = y.shape[0]\n","        output = model(x1, x2).view(-1)\n","        optim.zero_grad()\n","        loss = criterion(output, y)\n","        loss.backward()\n","        optim.step()\n","        total += batch\n","        sum_loss += batch*(loss.item())\n","    return sum_loss/total\n","\n","def valLoss(model, criterion, validDL):\n","    model.eval()\n","    total = 0\n","    sum_loss = 0\n","    correct = 0\n","    for x1, x2, y in validDL:\n","        current_batch_size = y.shape[0]\n","        out = model(x1, x2).view(-1)\n","        loss = criterion(out, y)\n","        sum_loss += current_batch_size*(loss.item())\n","        total += current_batch_size\n","        pred = torch.round(out)\n","        correct += (pred == y).float().sum().item()\n","    print(f\"Valid loss: {sum_loss/total}, Accuracy: {correct/total}\")\n","    return sum_loss/total, correct/total\n","\n","def trainLoop(model, epochs, lr=1e-4, wd=0.0):\n","    criterion = nn.BCEWithLogitsLoss()\n","    optim = getOptimizer(model, lr=lr, wd=wd)\n","    for i in range(epochs): \n","        loss = trainModel(model, criterion, optim, trainDL)\n","        print(\"Training loss: \", loss)\n","        with open(f'/content/gdrive/My Drive/Notebooks Colab/DNNModel4_e{i + 30 + 1}.pickle', 'wb') as handle:\n","            pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","            print(\"Model saved\")\n","        valLoss(model, criterion, validDL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sRu8g8VfubOj"},"source":["# Training execution"]},{"cell_type":"code","metadata":{"id":"QBR9eX64JlDQ"},"source":["batchSize = 1024\n","trainDL = DataLoader(trainDF, batch_size=batchSize, shuffle=True)\n","validDL = DataLoader(testDF, batch_size=batchSize, shuffle=True)\n","trainDL = DeviceDataLoader(trainDL, device)\n","validDL = DeviceDataLoader(validDL, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrJXF67qJ3mg"},"source":["trainLoop(model, epochs=30, lr=1e-4, wd=1e-6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0kSnAWAkFpD"},"source":["with open(f'/content/gdrive/My Drive/Notebooks Colab/DNNModel.pickle', 'wb') as handle:\n","    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    print(\"Model saved\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2xPhT6Bru70U"},"source":["# Submission"]},{"cell_type":"code","metadata":{"id":"heJ8LykimZQb"},"source":["batchSize = 1024\n","testDF = MalwareDataset(dfTest, pd.Series(np.zeros(dfTest.shape[0], dtype=np.uint8)), embeddedColNames)\n","testDL = DataLoader(testDF, batch_size=batchSize)\n","testDL = DeviceDataLoader(testDL, device)\n","preds = []\n","with torch.no_grad():\n","    for x1, x2, y in testDL:\n","        prob = model(x1, x2).view(-1)\n","        preds += prob.cpu().detach().numpy().tolist()\n","yRes = np.array(preds).reshape(-1)\n","submission = pd.read_csv('/content/gdrive/My Drive/Notebooks Colab/sample_submission.csv')\n","submission['HasDetections'] = yRes\n","submission.to_csv('/content/gdrive/My Drive/Notebooks Colab/DNNsubmission.csv', index=False)\n","print(submission.shape)\n","submission.head()"],"execution_count":null,"outputs":[]}]}